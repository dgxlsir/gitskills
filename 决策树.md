# 决策树

#### 决策树的构造

优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据

缺点：可能会产生过度匹配问题

适用数据类型：数值型和标称型

所有相同类型的数据均在一个数据子集中

#### 创建分支的伪代码函数

if so return 类标签

else 

​	寻找划分数据集的最好特征

​	划分数据集

​	创建分支节点

​		for 每个划分的子集

​			调用此函数，并添加返回结果到分支节点中

​	return 分支几点

#### 划分数据集

原则：将无序的数据变得更加有序

### 相关程序清单

#### 1 计算给定数据集的香农熵

```
from math import log
def calcShannonEnt(dataSet)
	numEntries = len(dataSet)
	labelCounts = {}
	for featVec in dataSet:
	currentLabel = featVec[-1]
		if currentLabel not in LabelCounts.keys():
	   		labelCounts[currentLabel] = 0
		labelCounts[currentLabel] += 1
	shannonEnt = 0.0
	for key in labelCounts:
		prob = float(labelCounts[key])/numEntries
		shannonEnt -= prob*log(pro,2)
	return shannonEnt
```

#### 2 划分数据集

1 按照给定特征划分数据集

```
def splitDataSet(dataSet, axis, value):
	retDataSet = []
	for featVec in dataSet:
		if featVec[axis] = value
			reducedFeatVec = featVec[:axis]
			reducedFeatVec.extend(featVec[axis+1:])
			retDataSet.append(reducedFeatVec)
	return retDataSet
```

2 选择最好的数据集划分方式

```
def chooseBestFeatureTopSplit(dataSet):
	numFeatures = len(dataSet[0]) - 1
	baseEntropy = calcShannonEnt(dataSet)
	bestInfoGain = 0.0
	bestFeature = -1
	for i in range(numFeatures):
		featList = [example[i] for example in dataSet]
		uniqueVals = set(featList)
		newEntropy = 0.0
		for value in uniqueVals:
			subDataSet = splitDataSet(dataSet, i, value)
			prob = len(subDataSet)/float(len(dataSet))
			newEntropy += prob * calcShannonEnt(subDataSet)
		inforGain = baseEntropy - newEntropy
		if (infoGain > bestInfoGain):
			bestInfoGain = infoGain
			bestFeature =i
	return bestFeatrue
```

#### 3 递归构建决策树

创建树的函数代码

```
def createTree(dataSet, labels):
	classList = [example[-1] for example in dataSet]
	if classList.count(classList[0]) == len(classList)
		return classList[0]
	if len(dataSet[0]) == 1:
		return majorityCnt(ClassList)
	bestFeat = chooseBestFeatureTopSplit(dataSet)
	bestFeatLabel = labels[bestFeat]
	myTree = {bestFeatLabel:{}}
	del(labels[bestFeat])
	featValues = [example[bestFeat] for example in dataSet]
	uniqueVals = set(featValues)
	for value in uniqueVals:
		subLabels = labels[:]
		myTree[bestFeatLabel][value] = createTree(splitDataSet\
							(dataSet, bestFeat, value), subLabels)
	return myTree
	
```

